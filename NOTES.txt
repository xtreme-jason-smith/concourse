TODO 

	- [x] perform proper `check` in `image_resource` image fetching

	- [x] container state transition

	- [ ] add label that identifies the set of pods as owned by the "worker"

		e.g., `org.concourse-ci.worker=blabla`

		this way, each "registration" takes care of itself
			(and we can deal with getting rid of leftovers from "stalled"  --> "retired")

	- [ ] worker lifecycle

		- [ ] retire?
			
			this makes sense... e.g., when it comes to getting a cluster out

			
		- [ ] land?

			e.g., when peforming a "known" maintenance?


		- [ ] stalled?
		


	- [ ] garbage collection of containers

		- currently, `report`, list destroying, destroy.
				|
				'--> we probably don't need to call an API endpoint

					==> refactored that part of the code into a "handles syncer"

						- can either be used in the API (as part of responding 
						  to requests), or other components


		sweeper:

			1. lists the containers that it has	 		(ok)
			2. reports those					(not yet)
			3. gathers the list of containers to destroy		(not yet)
			4. destroys each.					(ok)



	- [ ] prepare an issue documenting the work performed, what to do next,
	      the approach, etc

	- [ ] see if we can influence scheduling by putting annotations / labels
	      that identify the builds themselves

	- [ ] get back the exit status of the script executions

	- [ ] put step

	- [ ] test custom resource types

	- [ ] pod spec stuff

		- [ ] container limits
		- [ ] user in task definition
		- [ ] privileged
			- demonstrate how we're able to go from source to container
			  image without the need of any privileges (that's a very basic
			  thing we should be able to do nowadays)


	- [ ] add logs to where it's important to extract info from

	- [ ] multiple namespaces / clusters?

		- argocd: https://github.com/argoproj/argo-cd/issues/1673

		- spinnaker!  https://docs.armory.io/spinnaker-install-admin-guides/add-kubernetes-account/

			- each "worker" registers against the cluster by providing its own kubeconfig,
			  that has everything set to reach out to the cluster/target


				=> you create the svc account, token, etc, and then give us
				   the locations of the kubeconfigs.

					==> being a file on disk, that could come from a set of
					    secrets.


			use SelfSubjectAccessReview to determine if we can do what we want
			https://kubernetes.io/docs/reference/access-authn-authz/authorization/#checking-api-access


		- [ ] tags?
			
			==> we could leverage the notion of `teams` that we have
			ALREADY and leverage that in the form of a "pool" that
			selects based on that and tags


	- [ ] should thee worker regitration bring `nodeSelector` stuff?

	- [ ] caches?

	- [ ] fly hijack

	- [ ] fly execute

	- [ ] configure client-go's BPS

	- [ ] having non-k8s workers together
		- as a user, would you even choose that?

BENEFITS OF K8S 


	- [ ] if we DO use kubernetes CRDs for certain things (treat it as "THE
	  API" for control plane) Concourse-related, tools that have been built
	  around that ecosystem (e.g., vmware's octant) can than leverage that
	  (e.g., for visualization)

		=> e.g., if all inputs to concourse are k8s objects, you could
		have your tools that validates inputs to k8s as now validators
		to Concourse too

			-> even if `fly` acts just as a proxy for creating
			those objects.. :thinking:


	- [ ] how do we consume k8s secrets nowadays?
	  	(forgot)


	- [ ] automount of service accounts
		(better disable this thing)

			(what if you actually wanted a `task` to have a given
			serviceAccount mounted? :thinking:)
				- could we provide some "Extensions points"
				  somehow?
					- would break that nice abstraction we
					  have right now though

			--> you as the owner of a given namespace, could
			configure this stuff.

				-> because you can mutate your pod definitions
				w/ controllers, you can, e.g., inject a GPU



