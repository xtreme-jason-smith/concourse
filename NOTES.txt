Kubernetes runtime

### challenge

Without having Kubernetes as a native scheduling target, the execution of steps
in that environment become opaque to Kubernetes, which:

- i. makes impossible for operators to have first-class support for dealing with
  units of work that Concourse creates (containers)

- ii. forces those operators to grant much bigger permissions that they'd want
  to give to Concourse just for the sake of allowing workers to be able to do
  nested containerization.


### what would make this better

Being able to target Kuberetes with native workloads.



### proposal

- a "kubernetes target" maps to a "worker" in our data model

  - access to a "kubernetes target" is granted by giving to `web` either via
    `kubeconfig` (which contains the information about how to access that
    cluster's apiserver, with a token of a serviceaccount in that cluster that
    gives `web` permissions to target a specic namespace), or via service
    accounts if aiming at the same cluster as where the `web` pod lives

    - being "file-based", these could come from mounts into the `web` pod

    - when using`kubeconfig`, it's possible to target any kubernetes cluster
      (not necessarily the same as where the `web` pod lives)

      - the `web` pod does not need to be deployed in k8s (as access to those
	targets is granted through the kubeconfig files)

- Concourse "containers" (from our data model perspective) map to kubernetes
  pods

  - containers map to "concourse workers", which, in k8s, are kubernetes targets




### questions

> why kubeconfig instead of service account references?

using kubeconfig, we can have a single unit that brings all information that we
need with regards to getting access to a location to put kubernetes resources
into, as well as the credentials for doing so.

with a service account, we'd have the credentials, but not the "where"
(apiserver uri, certs, etc).






TODO 

	- [x] perform proper `check` in `image_resource` image fetching

	- [x] container state transition

	- [x] ignore task caches in list of artifact inputs

	- [x] put step

	- [ ] is there a `kpack` resource type? that'd be interesting
		
		... you don't even need one - as long as you can create the CRD
		.........

	- [x] task outputs

	- [ ] add label that identifies the set of pods as owned by the "worker"

		e.g., `org.concourse-ci.worker=blabla`

		this way, each "registration" takes care of itself
			(and we can deal with getting rid of leftovers from "stalled"  --> "retired")

	- [ ] worker lifecycle

		- [ ] retire?
			
			this makes sense... e.g., when it comes to getting a cluster out

			
		- [ ] land?

			e.g., when peforming a "known" maintenance?


		- [ ] stalled?
		


	- [ ] garbage collection of containers

		- currently, `report`, list destroying, destroy.
				|
				'--> we probably don't need to call an API endpoint

					==> refactored that part of the code into a "handles syncer"

						- can either be used in the API (as part of responding 
						  to requests), or other components


		sweeper:

			1. lists the containers that it has	 		(ok)
			2. reports those					(not yet)
			3. gathers the list of containers to destroy		(not yet)
			4. destroys each.					(ok)



	- [ ] prepare an issue documenting the work performed, what to do next,
	      the approach, etc

	- [ ] see if we can influence scheduling by putting annotations / labels
	      that identify the builds themselves

	- [ ] get back the exit status of the script executions

	- [ ] test custom resource types

	- [ ] pod spec stuff

		- [ ] container limits
		- [ ] user in task definition
		- [ ] privileged
			- demonstrate how we're able to go from source to container
			  image without the need of any privileges (that's a very basic
			  thing we should be able to do nowadays)


	- [ ] add logs to where it's important to extract info from

	- [ ] multiple namespaces / clusters?

		- argocd: https://github.com/argoproj/argo-cd/issues/1673

		- spinnaker!  https://docs.armory.io/spinnaker-install-admin-guides/add-kubernetes-account/

			- each "worker" registers against the cluster by providing its own kubeconfig,
			  that has everything set to reach out to the cluster/target


				=> you create the svc account, token, etc, and then give us
				   the locations of the kubeconfigs.

					==> being a file on disk, that could come from a set of
					    secrets.


			use SelfSubjectAccessReview to determine if we can do what we want
			https://kubernetes.io/docs/reference/access-authn-authz/authorization/#checking-api-access


		- [ ] tags?
			
			==> we could leverage the notion of `teams` that we have
			ALREADY and leverage that in the form of a "pool" that
			selects based on that and tags


	- [ ] should thee worker regitration bring `nodeSelector` stuff?

	- [ ] caches?

	- [ ] fly hijack

	- [ ] fly execute

	- [ ] configure client-go's BPS

	- [ ] having non-k8s workers together
		- as a user, would you even choose that?

BENEFITS OF K8S 


	- [ ] if we DO use kubernetes CRDs for certain things (treat it as "THE
	  API" for control plane) Concourse-related, tools that have been built
	  around that ecosystem (e.g., vmware's octant) can than leverage that
	  (e.g., for visualization)

		=> e.g., if all inputs to concourse are k8s objects, you could
		have your tools that validates inputs to k8s as now validators
		to Concourse too

			-> even if `fly` acts just as a proxy for creating
			those objects.. :thinking:


	- [ ] how do we consume k8s secrets nowadays?
	  	(forgot)


	- [ ] automount of service accounts
		(better disable this thing)

			(what if you actually wanted a `task` to have a given
			serviceAccount mounted? :thinking:)
				- could we provide some "Extensions points"
				  somehow?
					- would break that nice abstraction we
					  have right now though

			--> you as the owner of a given namespace, could
			configure this stuff.

				-> because you can mutate your pod definitions
				w/ controllers, you can, e.g., inject a GPU



